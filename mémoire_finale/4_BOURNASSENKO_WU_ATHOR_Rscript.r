# -*- coding: utf-8 -*-
"""R-Script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yq5qdSVhjy_6Wpeyid4tV5VbnnbxQ-w8

<font size="1">*Pour une meilleure lisibilité, nous conseillons de lire ce notebook dans Colab via ce lien : https://colab.research.google.com/drive/1yq5qdSVhjy_6Wpeyid4tV5VbnnbxQ-w8*</font>

# <font color='#40e0d0'>0 : Infos + Packages</font>

## Informations

Auteurs : Grégory Bournassenko(1), Axel Athor(2), Romain Wu(3) <br>
E-Mail (1) : gregory.bournassenko@etu.u-paris.fr <br>
E-Mail (2) : axel.athor@etu.u-paris.fr <br>
E-Mail (3) : romain.wu@etu.u-paris.fr <br>

---

Université de Paris, L3 économie & gestion, 2021-2022, S5 <br>
R-Script du mémoire d'économétrie <br>

---

Thème : Prix de l'immobilier (Ames, Iowa) <br>

---


![](https://cdn.pixabay.com/photo/2016/08/20/15/32/washington-d-1607766_960_720.jpg)

## Installation des packages
"""

# Vérifier rapidement si les packages sont déjà installés
require("nortest")
require("sandwich")
require("car")
require("lmtest")
require("modeldata")
require("leaps")
require("ggcorrplot")
require("DataExplorer")
require("leaflet")
require("tidyverse")
require("scales")
require("ggplot2")

install.packages("nortest") # Package pour tester la normalité
install.packages("sandwich") # Package pour corriger l'hétéroscédasticté
install.packages("car") # Package pour tester l'hétéroscédasticté
install.packages("lmtest") # Package pour tester l'hétéroscédasticté
install.packages("modeldata") # Package pour obtenir le dataset "Ames"
install.packages("leaps") # Package le choix de modèle, avec cp de Mallow et BIC
install.packages("ggcorrplot") # Package pour la visualisation des corrélations
install.packages("DataExplorer") # Package pour l'exploration des données
install.packages("leaflet") # Package pour visualiser la carte
install.packages("scales") # Package pour visualiser la carte
install.packages("tidyverse") # Package pour visualiser la carte (et autre)
install.packages("ggplot2") # Package pour la visualisation des données

"""## Chargement des packages"""

library(nortest)
library(sandwich)
library(car)
library(lmtest)
library(modeldata)
library(ggplot2)
library(leaps)
library(ggcorrplot)
library(DataExplorer)
library(tidyverse)
library(leaflet)
library(scales)
library(htmlwidgets)

"""# <font color='#40e0d0'>1 : Préparation + Nettoyage de données</font>

##Premier aperçu et nettoyage
Carte du dataset complet (sans nettoyage) : https://www.gregory798.educationhost.cloud/map.html
"""

data(ames) # Chargement du dataset "Ames" sur lequel nous travaillons
# write.csv2(ames, file = "ames.csv") Exporter la base de données (1 fois)
head(ames) # Vérification du dataset
summary(ames) # Premières statistiques descriptives
class(ames) # On remarque que ames n'est pas un DataFrame
# Nous le convertissons donc en un DataFrame :
ames = as.data.frame(ames)
class(ames) # Maintenant "ames" est un DataFrame
head(ames) # On peut donc visualiser ses premières lignes

# Après analyse du DataSet, on remarque qu'il y a plusieurs types de
# conditions de vente. Pour simplifier, nous ne prendrons que les ventes
# de type "normal". Pareillement, il existe plusieurs types de bâtiment.
# Nous prendrons uniquement les bâtiments pour une famille "OneFam",
# dans un souci de simplification et d'homogénéité.

dim(ames) # 2 930 X 74 : Il y a 2 930 observations dans ce DataSet
length(ames$Bldg_Type[ames$Bldg_Type == "OneFam"]) # Dont 2 425 "OneFam"
length(ames$Bldg_Type[ames$Bldg_Type != "OneFam"]) # Et 505 != "OneFam"
# 2 425 + 505 = 2930, le compte est bon


length(ames$Sale_Condition[ames$Sale_Condition == "Normal"]) # 2 413 "Normal"
length(ames$Sale_Condition[ames$Sale_Condition != "Normal"]) # 517 != "Normal"
# 2 413 + 517 = 2930, le compte est bon

# Nettoyage des données comme expliqué précédement :

ames = subset(ames,
              ames$Bldg_Type == "OneFam" &
              ames$Sale_Condition == "Normal")

dim(ames) # 2 002 X 74 : Premier nettoyage du DataSet
length(ames$Bldg_Type[ames$Bldg_Type != "OneFam"]) # 0 : parfait
length(ames$Sale_Condition[ames$Sale_Condition != "Normal"]) # 0 : parfait

"""##Visualisation de la carte (exécuter dans RStudio, ne marche pas dans Colab)
Carte du dataset nettoyé : https://www.gregory798.educationhost.cloud/map-clean.html
"""

nb.cols = 28
mycolors = colorRampPalette(RColorBrewer::brewer.pal(8, "Set2"))(nb.cols)

pal = colorFactor(
  palette = mycolors,
  levels = levels(ames$Neighborhood)
)

map = leaflet(ames) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(lng = ~Longitude,
                   lat = ~Latitude,
                   opacity = NULL,
                   radius = ~Sale_Price / 60000,
                   color = ~pal(Neighborhood),
                   group = ~Neighborhood,
                   popup = ~paste(Neighborhood, dollar(Sale_Price))) %>%
  addLayersControl(overlayGroups = ames$Neighborhood,
                   options = layersControlOptions(collapsed = FALSE))
                   
# On sauvegarde la carte dans le dossier courant
# Attention dans Google Colab les données partent vite
saveWidget(map, file="map.html")

"""## Correction du problème de nullité de la variance de 2 variables (suite au nettoyage)"""

# On identifie les variables qualitative à une (1) modalité
sapply(lapply(ames, unique), length) # Sale_Condition et Bldg_Type

# On enlève les factors à 1 level (sinon variance nulle...)
ames.without1F = subset(
  ames,
  select = -c(Sale_Condition,Bldg_Type)
)

dim(ames.without1F) # 2 002 x 72 maintenant

# On vérifie rapidement ce que donne un modèle avec toutes les variables :
summary(lm(log(Sale_Price) ~ ., ames.without1F))# AR2 = 0.9463

"""## Analyse des corrélations"""

# On extrait les variables quantitatives pour les analyser en premier
ames_num = select_if(ames.without1F, is.numeric)

# Matrice de corrélation pour les variables quantitatives
model.matrix(~0+., data=ames_num) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

# Pour la corrélation des variables qualitatives :
# 1 --> Extraire la variable expliquée
# 2 --> Extraire les variables qualitatiaves
# 3 --> Fusionner les variables qualitatives avec la variable expliquée
# 4 --> Afficher chaque matrices


# 1 --> Extraire la variable expliquée
sp = subset(ames.without1F, select = c("Sale_Price"))

# 2 --> Extraire les variables qualitatiaves
ames_fac = select_if(ames.without1F, is.factor)
dim(ames_fac) # Partition de datasets jusqu'à la 38e variable
fac1 = subset(ames_fac, select = c(1:5))
fac2 = subset(ames_fac, select = c(6:11))
fac3 = subset(ames_fac, select = c(12:17))
fac4 = subset(ames_fac, select = c(18:23))
fac5 = subset(ames_fac, select = c(24:29))
fac6 = subset(ames_fac, select = c(30:35))
fac7 = subset(ames_fac, select = c(36:38))

# 3 --> Fusionner les variables qualitatives avec la variable expliquée
fusion1 = cbind(fac1, sp)
fusion2 = cbind(fac2, sp)
fusion3 = cbind(fac3, sp)
fusion4 = cbind(fac4, sp)
fusion5 = cbind(fac5, sp)
fusion6 = cbind(fac6, sp)
fusion7 = cbind(fac7, sp)

# 4 --> Afficher chaque matrices
# Matrice de corrélation pour les variables qualitatives (Partie 1)
model.matrix(~0+., data=fusion1) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
# Matrice de corrélation pour les variables qualitatives (Partie 2)
model.matrix(~0+., data=fusion2) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
# Matrice de corrélation pour les variables qualitatives (Partie 3)
model.matrix(~0+., data=fusion3) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
# Matrice de corrélation pour les variables qualitatives (Partie 4)
model.matrix(~0+., data=fusion4) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
# Matrice de corrélation pour les variables qualitatives (Partie 5)
model.matrix(~0+., data=fusion5) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
# Matrice de corrélation pour les variables qualitatives (Partie 6)
model.matrix(~0+., data=fusion6) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
# Matrice de corrélation pour les variables qualitatives (Partie 7)
model.matrix(~0+., data=fusion7) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

"""##Analyse des quartiers et fondations"""

# Nombre de ventes et prix médian par quartiers
ames.without1F %>%
  group_by(Neighborhood) %>%
  summarize(count = n(), median_price = median(Sale_Price)) %>%
  ungroup() %>%
  mutate(Neighborhood = fct_reorder(Neighborhood, count)) %>%
  ggplot( aes(count, Neighborhood, fill = median_price)) + geom_col()

# Nombre de ventes et prix médian par type de fondation
ames.without1F %>%
  group_by(Foundation) %>%
  summarize(count = n(), median_price = median(Sale_Price)) %>%
  ungroup() %>%
  mutate(Foundation = fct_reorder(Foundation, count)) %>%
  ggplot( aes(count, Foundation, fill = median_price)) + geom_col()

"""##Statistiques descriptives <br>
Fin du code : exécuter dans RStudio (ne marche pas dans Colab) <br>
Rapport DataExplorer : https://drive.google.com/file/d/1kHehQ887L_IAdVUUvlG6i5lHSgfX91hb/view?usp=sharing <br>
Rapport SummaryTools : https://drive.google.com/file/d/1akz54BodixRrpr04thZ1DWJSwLRigeQL/view?usp=sharing

"""

# Sélection des variables corrélées
amesUse = subset(ames.without1F, 
                 select = c("Sale_Price",
                            "Gr_Liv_Area",
                            "Garage_Cars",
                            "Total_Bsmt_SF",
                            "Year_Built",
                            "Full_Bath",
                            "Foundation",
                            "Neighborhood"))

# Génération d'un rapport de statistiques descriptives dans le dossier courant
create_report(amesUse, y = "Sale_Price")

# Tableau 1 : Foundation vs Sale Price (summary)
tapply(ames.without1F$Sale_Price, ames.without1F$Foundation, summary)

# Tableau 2 : Neighborhood vs Foundations
table(ames.without1F$Neighborhood, ames.without1F$Foundation)

# Tableau 3 : Neighborhood vs Sale Price (summary)
tapply(ames.without1F$Sale_Price, ames.without1F$Neighborhood, summary)

# Executer dans RStudio
# library(summarytools)
# print(dfSummary(amesUse), method = 'browser')

"""# <font color='#40e0d0'>2 : Modèles + Résultats</font>

## Transformation du $SalePrice$ en $log(SalePrice)$
"""

# Sale_Price (sans log)
ggplot(ames, aes(Sale_Price)) + geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dnorm, color = "red", 
                args = list(mean = mean(ames$Sale_Price), 
                            sd = sd(ames$Sale_Price)))
# On fait un test d'Anderson-Darling pour vérifier la normalité
ad.test(unique(ames.without1F$Sale_Price)) # p-value < 0.05 : Rejet H0


# log(Sale_Price)
ggplot(ames, aes(log(Sale_Price))) + geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dnorm, color = "red", 
                args = list(mean = mean(log(ames$Sale_Price)),
                            sd = sd(log(ames$Sale_Price)))) + 
  theme(plot.title = element_text(size = 11))

# On fait un test d'Anderson-Darling pour vérifier la normalité
ad.test(unique(log(ames.without1F$Sale_Price)))# p-value > 0.05 : Non Rejet H0

"""##<font color='#ffbdd8'>$log(SalePrice) = \beta_0 + \beta_1GrLivArea + u$"""

m1 = lm(log(Sale_Price) ~ Gr_Liv_Area, ames.without1F)
summary(m1) # Adjusted R-squared:  0.5999

"""##<font color='#ff8787'>$log(SalePrice) = \beta_0 + \beta_1GrLivArea + \beta_2GarageCars + \beta_3TotalBasementSF + \beta_4YearBuilt + \beta_5FullBath + \beta_6Foundation + \beta_7Neighborhood + u$"""

m2 = lm(log(Sale_Price) ~ Gr_Liv_Area + Garage_Cars + Total_Bsmt_SF + Year_Built
+ Full_Bath + Foundation + Neighborhood, ames.without1F)
summary(m2) # Adjusted R-squared:  0.8654

"""##<font color='#ffaf6e'>$log(SalePrice) = \beta_0 + \beta_1MsSubClass + ... + \beta_{71}Latitude + u$"""

m3 = lm(log(Sale_Price) ~ ., ames.without1F)
summary(m3) # Adjusted R-squared:  0.9463

"""##<font color='#f7c768'>$log(SalePrice) = \beta_0 + \beta_1LotFrontage 
+ \beta_2LotArea + \beta_3YearBuilt 
+ \beta_4YearRemodAdd + \beta_5MasVnrArea 
+ \beta_6BsmtFinSF1 + \beta_7BsmtFinSF2 
+ \beta_8BsmtUnfSF + \beta_9TotalBsmtSF 
+ \beta_{10}FirstFlrSF + \beta_{11}SecondFlrSF 
+ \beta_{12}BsmtFullBath + \beta_{13}BedroomAbvGr 
+ \beta_{14}KitchenAbvGr + \beta_{15}TotRmsAbvGrd 
+ \beta_{16}Fireplaces + \beta_{17}GarageCars 
+ \beta_{18}GarageArea + \beta_{19}WoodDecSF 
+ \beta_{20}ScreenPorch + \beta_{21}PoolArea + u$
"""

# Recherche du meilleur modèle pour les variables quantitatives uniquement
# car trop long (impossible) pour les variables qualitatives
bestmodel  = regsubsets(log(Sale_Price) ~ .,
             data = ames_num,
             nbest = 1,
             nvmax = NULL,
             force.in = NULL, force.out = NULL,
             method = "exhaustive",
             really.big = TRUE)

# Le summary nous donne les meilleurs combinaisons 
# de modèle pour 1 à n variables
s = summary(bestmodel)
s
# On stocke les résulat du R2 ajusté , du cp de Mallow et du BIC
results = data.frame(ADJR2 = s$adjr2, CP = s$cp, BIC = s$bic)
# On cherche parmis ces résulats le modèle qui répond au critères suivants :
# R2A élevé, cp faible, BIC faible
print(results) # 21 : R2A = 0.8845, cp = 23.8734, BIC = -4 175.918
# On trouve que le modèle 21 est le meilleur compromis entre ces 3 critères
# alors on affiche ses variables :
b = as.data.frame(coef(bestmodel, 21))
print(row.names(b))

# Les "meilleures" variables (quantitatives) sont :
# "Lot_Frontage"   "Lot_Area"       "Year_Built"    
# "Year_Remod_Add" "Mas_Vnr_Area"   "BsmtFin_SF_1"   "BsmtFin_SF_2"  
# "Bsmt_Unf_SF"    "Total_Bsmt_SF"  "First_Flr_SF"   "Second_Flr_SF" 
# "Bsmt_Full_Bath" "Bedroom_AbvGr"  "Kitchen_AbvGr"  "TotRms_AbvGrd" 
# "Fireplaces"     "Garage_Cars"    "Garage_Area"    "Wood_Deck_SF"  
# "Screen_Porch"   "Pool_Area"

m4 = lm(log(Sale_Price) ~ 
Lot_Frontage + Lot_Area + Year_Built 
+ Year_Remod_Add + Mas_Vnr_Area + BsmtFin_SF_1
+ BsmtFin_SF_2 + Bsmt_Unf_SF + Total_Bsmt_SF 
+ First_Flr_SF + Second_Flr_SF 
+ Bsmt_Full_Bath + Bedroom_AbvGr + Kitchen_AbvGr 
+ TotRms_AbvGrd + Fireplaces + Garage_Cars 
+ Garage_Area + Wood_Deck_SF
+ Screen_Porch + Pool_Area
, ames.without1F)

summary(m4) #Adjusted R2: 0.8845, on remarque le même qu'avec regsubsets

"""# <font color='#40e0d0'>3 :Tests d'hypothèses</font>

## **Hétéroscédasticté** <br>
(Correction par la méthode de White)

### Modèle 1
"""

# On fait 2 tests d'hétéroscédasticté (pour être sûr)
ncvTest(m1) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
bptest(m1) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
# On affiche les résidus standardisés VS les y prédits
# Pour vérifier si la droite est horizontale
plot(m1, which = 3) # La droite n'est pas horizontale
# On vérifie aussi la constance de la variance entre les résidus et les
# les variables douteuse : ici GrLivArea
plot(ames.without1F$Gr_Liv_Area, m1$residuals)
abline(h = mean(m1$residuals), col = "red")
# Alors on utilise les écart-types corrigés par la matrice de White
coeftest(m1, vcov = vcovHC(m1, "HC0"))

"""### Modèle 2"""

# On fait 2 tests d'hétéroscédasticté (pour être sûr)
ncvTest(m2) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
bptest(m2) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
# On affiche les résidus standardisés VS les y prédits
# Pour vérifier si la droite est horizontale
plot(m2, which = 3) # La droite n'est pas horizontale
# On vérifie aussi la constance de la variance entre les résidus et les
# les variables douteuse : ici Neighborhood, Foundation et Year_Built
plot(ames.without1F$Neighborhood, m2$residuals)
abline(h = mean(m2$residuals), col = "red")
plot(ames.without1F$Foundation, m2$residuals)
abline(h = mean(m2$residuals), col = "red")
plot(ames.without1F$Year_Built, m2$residuals)
abline(h = mean(m2$residuals), col = "red")
# Alors on utilise les écart-types corrigés par la matrice de White
coeftest(m2, vcov = vcovHC(m2, "HC0"))

"""### Modèle 3"""

# On fait 2 tests d'hétéroscédasticté (pour être sûr)
ncvTest(m3) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
bptest(m3) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
plot(m3, which = 3) # Droite non-horizontale
# On vérifie aussi la constance de la variance entre les résidus et les
# les variables douteuse : ici Neighborhood, Foundation
plot(ames.without1F$Neighborhood, m3$residuals)
abline(h = mean(m3$residuals), col = "red")
plot(ames.without1F$Foundation, m3$residuals)
abline(h = mean(m3$residuals), col = "red")
# Alors on utilise les écart-types corrigés par la matrice de White
coeftest(m3, vcov = vcovHC(m3, "HC0"))

"""### Modèle 4"""

# On fait 2 tests d'hétéroscédasticté (pour être sûr)
ncvTest(m4) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
bptest(m4) # p < 0.001 Rejet de H0 (variance constante) : hétéroscédasticté
# On affiche les résidus standardisés VS les y prédits
# Pour vérifier si la droite est horizontale
plot(m4, which = 3) # La droite n'est pas horizontale
# On vérifie aussi la constance de la variance entre les résidus et les
# les variables douteuse : ici Neighborhood, Foundation
plot(ames.without1F$Neighborhood, m4$residuals)
abline(h = mean(m4$residuals), col = "red")
plot(ames.without1F$Foundation, m4$residuals)
abline(h = mean(m4$residuals), col = "red")
# Alors on utilise les écart-types corrigés par la matrice de White
coeftest(m4, vcov = vcovHC(m4, "HC0"))

"""## **Normalité des résidus**

### Modèle 1
"""

shapiro.test(m1$residuals) # p-value < 0.001 : rejet de H0 = non-normalité
hist(residuals(m1), freq=F) # Visualisation de la distribution
lines(density(residuals(m1)), col = "red")

"""### Modèle 2"""

shapiro.test(m2$residuals) # p-value < 0.001 : rejet de H0 = non-normalité
hist(residuals(m2), freq=F) # Visualisation de la distribution
lines(density(residuals(m2)), col = "red")

"""### Modèle 3"""

shapiro.test(m3$residuals) # p-value < 0.001 : rejet de H0 = non-normalité
hist(residuals(m3), freq=F) # Visualisation de la distribution
lines(density(residuals(m3)), col = "red")

"""### Modèle 4"""

shapiro.test(m4$residuals) # p-value < 0.001 : rejet de H0 = non-normalité
hist(residuals(m4), freq=F) # Visualisation de la distribution
lines(density(residuals(m4)), col = "red")

"""## **Multicolinéarité**

### Modèle 1
"""

# Inutile car une seule variable explicative

"""### Modèle 2"""

vif(m2) # Aucun VIF > 5 : pas de multicolinéarité

"""### Modèle 3"""

vif(m3) # Erreur : normal, il y a toutes les variables

"""### Modèle 4"""

as.data.frame(vif(m4)) # Garage_Cars VIF > 5 : on le savait grâce à la matrice de corr.

"""## **Autocorrélation**

### Modèle 1
"""

durbinWatsonTest(m1) # p-value < 0.001 : Les résidus sont dépendants
dwtest(m1) # p-value < 0.001 : Les résidus sont dépendants

"""### Modèle 2"""

durbinWatsonTest(m2) # p-value < 0.001 : Les résidus sont dépendants
dwtest(m2) # p-value < 0.001 : Les résidus sont dépendants

"""### Modèle 3"""

durbinWatsonTest(m3) # p-value < 0.001 : Les résidus sont dépendants
dwtest(m3) # p-value > 0.05 : Les résidus sont indépendants

"""### Modèle 4"""

durbinWatsonTest(m4) # p-value < 0.001 : Les résidus sont dépendants
dwtest(m4) # p-value < 0.001 : Les résidus sont dépendants

"""## **Linéarité**

### Modèle 1
"""

raintest(m1) # p-value > 0.05 : La relation est linéaire
plot(m1, which = 1) # Droite peu horizontale

"""### Modèle 2"""

raintest(m2) # p-value > 0.05 : La relation est linéaire
plot(m2, which = 1) # Droite assez horizontale

"""### Modèle 3"""

raintest(m3) # p-value < 0.05 : La relation est non-linéaire
plot(m3, which = 1) # Droite assez horizontale

"""### Modèle 4"""

raintest(m4) # p-value > 0.05 : La relation est linéaire
plot(m4, which = 1) # Droite assez horizontale

"""## **Valeurs influentes**

### Modèle 1
"""

plot(m1, which = 5) # Aucune valeur influente

"""### Modèle 2"""

plot(m2, which = 5) # Aucune valeur influente

"""### Modèle 3"""

plot(m3, which = 5) # Pas vraiment d'intérêt

"""### Modèle 4"""

plot(m4, which = 5) # Aucune valeur influente

"""# <font color='#40e0d0'>Bonus : Shiny App</font> <br>
https://kyme37-gr0gory-b.shinyapps.io/Ames/
"""

data("ames")

m2.2 = lm(Sale_Price ~ Gr_Liv_Area + Garage_Cars + Total_Bsmt_SF + Year_Built
+ Full_Bath, ames)

summary(m2.2)

mean(ames$Gr_Liv_Area)
mean(ames$Garage_Cars)
mean(ames$Total_Bsmt_SF)
mean(ames$Year_Built)
mean(ames$Full_Bath)

library(shiny)
library(shinythemes)
  ui <- fluidPage(theme = shinytheme("darkly"),
    navbarPage(
      "Mémoire économétrie - Grégory BOURNASSENKO, Romain WU, Axel ATHOR",
      tabPanel("Menu",
               sidebarPanel(
                 h1("Variables explicatives :"),
                 sliderInput("sh", "Surface habitable (pieds-carrés) :",
                             min = 0, max = 10000, value = 1499.69
                 ),
                 sliderInput("nv", "Nombre de voitures :",
                             min = 0, max = 10, value = 2
                 ),
                 sliderInput("sss", "Surface du sous-sol (pieds-carrés) :",
                             min = 0, max = 10000, value = 1051.25
                 ),
                 sliderInput("ac", "Année de construction :",
                             min = 1900, max = 2100, value = 1971.35
                 ),
                 sliderInput("nb", "Nombre de baignoires complètes :",
                             min = 0, max = 10, value = 1
                 ),
                 
               ), 
               mainPanel(
                            h1("Variable expliquée :"),
                            h4("Prix de vente"),
                            verbatimTextOutput("txtout"),
               )
      ),
    ) 
  )   
  server <- function(input, output) {
    
    output$txtout <- renderText({
      input$sh*73.89+input$nv*2000.1+input$sss*44.98+
        (input$ac*692.6-6353*input$nb-1368000)
    })
  } 
  shinyApp(ui = ui, server = server)